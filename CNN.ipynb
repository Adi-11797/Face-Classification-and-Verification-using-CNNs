{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBITN0M_LKds"
      },
      "source": [
        "# HW2P2: Face Classification and Verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH4P-HzLRQs"
      },
      "source": [
        "Congrats on coming to the second homework in 11785: Introduction to Deep Learning. This homework significantly longer and tougher than the previous homework. You have 2 sub-parts as outlined below. Please start early! \n",
        "\n",
        "\n",
        "*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7000 identities\n",
        "*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities\n",
        "\n",
        "For this HW, you only have to write code to implement your model architecture. Everything else has been provided for you, on the pretext that most of your time will be used up in developing the suitable model architecture for achieving satisfactory performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1B_m84_cU6c"
      },
      "source": [
        "Common errors which you may face in this homeworks (because of the size of the model)\n",
        "\n",
        "\n",
        "*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instructions to run code: All cells need to be run!\n",
        "\n",
        "Ablations Strategies: \n",
        "\n",
        "1) Architectures considered:\n",
        "- ResNet 50  (Easy implementation - Low cutoff reached)\n",
        "- ResNet 101 (Allowed me to reach Medium Cutoff sucesfully, however, improvements thereafter were quite tedious and required a reconsideration of architecture)\n",
        "- ConvNeXt   (Best Performance - highly improved training accuracy achieved within 15 epochs)\n",
        "\n",
        "2) Epochs: \n",
        "- Training for longer epochs improved performance, however, beyond a certain number of epochs (75-85), the tradeoff in performance and resource consumption was not beneficial.\n",
        "\n",
        "3) Hyperparameters: \n",
        "* Learning Rate tuning was not required, 0.1 LR achieved the high cutoff requirement\n",
        "* Batch Size was experimented at different values from 16-128. Finally a batch_size of 64 was selected due its acceptable performance for compute units consumed. Although lower batch sizes notably improved performance.\n",
        "\n",
        "4) Data loading scheme:\n",
        "* RandomHorizontalFlip(0.5) - increasing value of flip marginally increased performance\n",
        "* ColorJitter(brightness = 0.65, contrast = 0.45, saturation = 0.55)\n",
        "* RandomGrayscale(p=0.2)\n",
        "* RandomRotation(degrees=(-30, 30))\n",
        "* RandomPerspective(distortion_scale=0.25, p=0.25) - minimal impact observed\n",
        "* RandAugment\n",
        "* GaussianBlur(kernel_size=(3, 3), sigma=(0.1,.2))\n",
        "  - Small Gaussian Blur worked in the initial stages, however, in the final code iteration, it did not improve performance, hence was removed\n",
        "* Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  - Normalization was not fruitful for this dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdoDIKWOMF59"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jza7lwiScUhb",
        "outputId": "d0e62c94-0cdd-431c-9984-a6ccc7650cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Nov  2 14:57:41 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    38W / 300W |   2907MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # to see what GPU you have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTxfd_nqFnL9"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5QVQoeC1MyL"
      },
      "outputs": [],
      "source": [
        "# !pip install poutyne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLEd0gdPbSc",
        "outputId": "e2cc4a11-bc0f-4639-946d-8fc151cc980a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision #This library is used for image-based operations (Augmentations)\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "#import wandb\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from poutyne import set_seeds\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRz9et3SZnbO",
        "outputId": "bde3cdc1-d5ff-4c51-c703-616ddecb87a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # Link your drive if you are a colab user\n",
        "drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it her\n",
        "\n",
        "# import os.path as path \n",
        "# if not path.exists(\"/content/drive\"):\n",
        "#     !sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "#     !sudo apt-get update -qq 2>&1 > /dev/null\n",
        "#     !sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "#     !google-drive-ocamlfuse\n",
        "\n",
        "#     !sudo apt-get install -qq w3m # to act as web browser \n",
        "#     !xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "#     %cd /content\n",
        "#     !mkdir drive\n",
        "#     %cd drive\n",
        "#     !mkdir MyDrive\n",
        "#     %cd ..\n",
        "#     %cd ..\n",
        "#     !google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oxQNl-YVWHc"
      },
      "source": [
        "# TODOs\n",
        "As you go, please read the code and keep an eye out for TODOs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scOnMklwWBY6"
      },
      "source": [
        "# Download Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BksgPdkQwwb",
        "outputId": "e9fc35cd-5a56-4f61-cd7c-76c941e6ec8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 4.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73276 sha256=a0a3ee86d1c3bb2c45c35be89044f60bb88e2497293b9f7f9d7b9f5625bb2dd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/f7/d8/c3902cacb7e62cb611b1ad343d7cc07f42f7eb76ae3a52f3d1\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"ripcurl11\",\"key\":\"a924e45910075179ad325ad28d952008\"}') \n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oFjaJTaRjT7",
        "outputId": "38c07e50-1cc1-4336-d06d-01080a19b9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-f22-hw2p2-classification.zip to /content\n",
            "100% 2.36G/2.37G [01:21<00:00, 41.2MB/s]\n",
            "100% 2.37G/2.37G [01:21<00:00, 31.1MB/s]\n",
            "Downloading 11-785-f22-hw2p2-verification.zip to /content\n",
            "100% 16.8M/16.8M [00:01<00:00, 27.4MB/s]\n",
            "100% 16.8M/16.8M [00:01<00:00, 17.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11-785-f22-hw2p2-classification\n",
        "!unzip -qo '11-785-f22-hw2p2-classification.zip' -d '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11-785-f22-hw2p2-verification\n",
        "!unzip -qo '11-785-f22-hw2p2-verification.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O68hT27SXClj"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S7qpMxG0XCJz"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 64, # Increase this if your GPU can handle it\n",
        "    'lr': 0.1,\n",
        "    'epochs': 85, # 10 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
        "    \n",
        "    ##### Include other parameters as needed.\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSeiKHYrM-6b"
      },
      "source": [
        "# Classification Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmRX5omaNDEZ",
        "outputId": "cd7b335e-578d-4430-bb6a-e37dcd1c9e37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# np.random.seed(42)\n",
        "# torch.manual_seed(42)\n",
        "\n",
        "DATA_DIR = '/content/data/11-785-f22-hw2p2-classification/'####################### TODO: Path where you have downloaded the data\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"classification/train\") \n",
        "VAL_DIR = os.path.join(DATA_DIR, \"classification/dev\")\n",
        "TEST_DIR = os.path.join(DATA_DIR, \"classification/test\")\n",
        "\n",
        "# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.RandomHorizontalFlip(0.5),\n",
        "                torchvision.transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation = 0.2),\n",
        "                torchvision.transforms.RandomGrayscale(p=0.2),\n",
        "                torchvision.transforms.RandAugment(),\n",
        "                #torchvision.transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                #torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ])\n",
        "# Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n",
        "\n",
        "# But there are some transforms which are performed after ToTensor() : e.g - Normalization\n",
        "# Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n",
        "\n",
        "val_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, transform = train_transforms)\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR, transform = val_transforms)\n",
        "# You should NOT have data augmentation on the validation set. Why?\n",
        "\n",
        "# train_indices, valid_test_indices = train_test_split(np.arange(len(dataset)),\n",
        "#                                                     train_size=0.4,\n",
        "#                                                     stratify=dataset.targets,\n",
        "#                                                     random_state=42)\n",
        "# # We take 20% for the validation dataset and 20% for the test dataset\n",
        "# # (i.e. 50% of the remaining 40%).\n",
        "# valid_indices, test_indices = train_test_split(valid_test_indices,\n",
        "#                                             train_size=0.5,\n",
        "#                                             stratify=np.asarray(dataset.targets)[valid_test_indices],\n",
        "#                                             random_state=42)\n",
        "\n",
        "# train_dataset = Subset(dataset, train_indices)\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = config['batch_size'], \n",
        "                                           shuffle = True,num_workers = 4, pin_memory = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = config['batch_size'], \n",
        "                                         shuffle = False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SqSR063BGE2e"
      },
      "outputs": [],
      "source": [
        "# You can do this with ImageFolder as well, but it requires some tweaking\n",
        "class ClassificationTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir   = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n",
        "        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fVLB41KtGC2o"
      },
      "outputs": [],
      "source": [
        "test_dataset = ClassificationTestDataset(TEST_DIR, transforms = val_transforms) #Why are we using val_transforms for Test Data?\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n",
        "                         drop_last = False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4t8eU9gY0Jy",
        "outputId": "7871ad8e-f712-4ddd-da00-ce89b370d910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes:  7000\n",
            "No. of train images:  140000\n",
            "Shape of image:  torch.Size([3, 224, 224])\n",
            "Batch size:  64\n",
            "Train batches:  2188\n",
            "Val batches:  547\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of classes: \", len(train_dataset.classes))\n",
        "print(\"No. of train images: \", train_dataset.__len__())\n",
        "print(\"Shape of image: \", train_dataset[0][0].shape)\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train batches: \", train_loader.__len__())\n",
        "print(\"Val batches: \", val_loader.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# Very Simple Network (for Mandatory Early Submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class Network(torch.nn.Module):\n",
        "#     \"\"\"\n",
        "#     The Very Low early deadline architecture is a 4-layer CNN.\n",
        "\n",
        "#     The first Conv layer has 64 channels, kernel size 7, and stride 4.\n",
        "#     The next three have 128, 256, and 512 channels. Each have kernel size 3 and stride 2.\n",
        "    \n",
        "#     Think about strided convolutions from the lecture, as convolutioin with stride= 1 and downsampling.\n",
        "#     For stride 1 convolution, what padding do you need for preserving the spatial resolution? \n",
        "#     (Hint => padding = kernel_size // 2) - Why?)\n",
        "\n",
        "#     Each Conv layer is accompanied by a Batchnorm and ReLU layer.\n",
        "#     Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d.\n",
        "#     Then, remove (Flatten?) these trivial 1x1 dimensions away.\n",
        "#     Look through https://pytorch.org/docs/stable/nn.html \n",
        "    \n",
        "#     TODO: Fill out the model definition below! \n",
        "\n",
        "#     Why does a very simple network have 4 convolutions?\n",
        "#     Input images are 224x224. Note that each of these convolutions downsample.\n",
        "#     Downsampling 2x effectively doubles the receptive field, increasing the spatial\n",
        "#     region each pixel extracts features from. Downsampling 32x is standard\n",
        "#     for most image models.\n",
        "\n",
        "#     Why does a very simple network have high channel sizes?\n",
        "#     Every time you downsample 2x, you do 4x less computation (at same channel size).\n",
        "#     To maintain the same level of computation, you 2x increase # of channels, which \n",
        "#     increases computation by 4x. So, balances out to same computation.\n",
        "#     Another intuition is - as you downsample, you lose spatial information. We want\n",
        "#     to preserve some of it in the channel dimension.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, num_classes=7000):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.backbone = torch.nn.Sequential(\n",
        "#             # TODO\n",
        "#             torch.nn.Conv2d(3, 64, kernel_size=7, stride=4, padding=3, bias=False),\n",
        "#             torch.nn.BatchNorm2d(64),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "#             torch.nn.BatchNorm2d(128),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "#             torch.nn.BatchNorm2d(256),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "#             torch.nn.BatchNorm2d(512),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.AdaptiveAvgPool2d((1, 1)), # For each channel, collapses (averages) the entire feature map (height & width) to 1x1\n",
        "#             torch.nn.Flatten(), # the above ends up with batch_size x 64 x 1 x 1, flatten to batch_size x 64\n",
        "#             ) \n",
        "        \n",
        "#         self.cls_layer = torch.nn.Sequential(\n",
        "#             torch.nn.Linear(512, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def weight_init(m):\n",
        "#       if isinstance(m, torch.nn.Linear):\n",
        "#         torch.nn.init.kaiming_uniform_(m.weight)\n",
        "#       elif isinstance(m, torch.nn.Conv2d):\n",
        "#         torch.nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "    \n",
        "#     def forward(self, x, return_feats=False):\n",
        "#         \"\"\"\n",
        "#         What is return_feats? It essentially returns the second-to-last-layer\n",
        "#         features of a given image. It's a \"feature encoding\" of the input image,\n",
        "#         and you can use it for the verification task. You would use the outputs\n",
        "#         of the final classification layer for the classification task.\n",
        "\n",
        "#         You might also find that the classification outputs are sometimes better\n",
        "#         for verification too - try both.\n",
        "#         \"\"\"\n",
        "#         feats = self.backbone(x)\n",
        "#         out = self.cls_layer(feats)\n",
        "\n",
        "#         if return_feats:\n",
        "#             return feats\n",
        "#         else:\n",
        "#             return out\n",
        "            \n",
        "# #model = Network().to(device)\n",
        "# #summary(model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ConNeXt Network (High Cutoff Submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JigcfcAAtgww"
      },
      "source": [
        "ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPjUEB1qZEG0",
        "outputId": "4c4061f3-a6d7-42ef-c01c-514360069985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4, 96, 3], [4, 192, 3], [4, 384, 9], [4, 768, 3]]\n",
            "4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): Network(\n",
              "    (layers): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (2): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (3): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
              "      )\n",
              "      (5): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (6): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (7): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (8): Sequential(\n",
              "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
              "      )\n",
              "      (9): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (10): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (11): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (12): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (13): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (14): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (15): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (16): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (17): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (18): Sequential(\n",
              "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
              "      )\n",
              "      (19): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
              "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (20): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
              "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "      (21): BasicConvNeXtBlock(\n",
              "        (conv_1): Sequential(\n",
              "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
              "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv_2): Sequential(\n",
              "          (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): GELU(approximate=none)\n",
              "        )\n",
              "        (conv_3): Sequential(\n",
              "          (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (drop_path): StochasticDepth(p=0.1, mode=batch)\n",
              "      )\n",
              "    )\n",
              "    (embeddings): Sequential(\n",
              "      (0): AdaptiveAvgPool2d(output_size=1)\n",
              "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): Flatten(start_dim=1, end_dim=-1)\n",
              "    )\n",
              "    (cls): Linear(in_features=768, out_features=7000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class BasicConvNeXtBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, exp_ratio, dropout=0.1):\n",
        "    super().__init__()\n",
        "    hidden_dim = in_channels * exp_ratio\n",
        "\n",
        "    # Referring to an article on ConvNeXt from https://towardsdatascience.com/implementing-convnext-in-pytorch-7e37a67abba6\n",
        "    # specific reference to bottleneck block in convnext-14.py \n",
        "\n",
        "    # narrow -> wide (with depth-wise and bigger kernel)\n",
        "    self.conv_1 = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3, groups=in_channels),\n",
        "        # torch.nn.GroupNorm((num_groups=1, num_channels=in_channels),\n",
        "        torch.nn.BatchNorm2d(in_channels)\n",
        "    )\n",
        "\n",
        "    # wide -> wide\n",
        "    self.conv_2 = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0),\n",
        "        #torch.nn.ReLU(),\n",
        "        #torch.nn.LeakyReLU(),\n",
        "        #torch.nn.PReLU(),\n",
        "        torch.nn.GELU(),\n",
        "    )\n",
        "\n",
        "    # wide -> narrow\n",
        "    self.conv_3 = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "    )\n",
        "\n",
        "    self.drop_path = torchvision.ops.StochasticDepth(p=dropout, mode=\"batch\")\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv_1(x)\n",
        "    out = self.conv_2(out)\n",
        "    out = self.conv_3(out)\n",
        "    x = x + self.drop_path(out)\n",
        "    return x\n",
        "    \n",
        "class Network(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes=7000):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.layers = []\n",
        "\n",
        "    # Defined ConvNeXt Stage Configurations from Facebook Research (Tiny ConvNeXt implementation)\n",
        "    # def convnext_tiny(pretrained=False,in_22k=False, **kwargs):\n",
        "    #   model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    self.stage_config = [\n",
        "        # Expansion Ratio = 4, No. of Channels, Depth\n",
        "        [4, 96, 3],\n",
        "        [4, 192, 3],\n",
        "        [4, 384, 9],\n",
        "        [4, 768, 3],\n",
        "    ]\n",
        "\n",
        "    print(self.stage_config)\n",
        "    print(len(self.stage_config))\n",
        "\n",
        "    # Stem: the first layer in the model that does the heavy downsampling of the input image.\n",
        "    stem = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(3, self.stage_config[0][1], kernel_size=4, stride=4),\n",
        "        torch.nn.BatchNorm2d(96)) \n",
        "\n",
        "    self.layers.append(stem)\n",
        "  \n",
        "    for i in range(0, len(self.stage_config)):\n",
        "      exp_ratio, in_channels, num_blocks = self.stage_config[i]\n",
        "\n",
        "      for j in range(0, num_blocks):\n",
        "        self.layers.append(BasicConvNeXtBlock(self.stage_config[i][1], self.stage_config[i][1], self.stage_config[i][0], dropout=0.1))\n",
        "      \n",
        "      if i < len(self.stage_config)-1:\n",
        "        # Intermediate downsampling conv layers\n",
        "        down_smpl = torch.nn.Sequential(\n",
        "          torch.nn.BatchNorm2d(self.stage_config[i][1]),\n",
        "          torch.nn.Conv2d(self.stage_config[i][1], self.stage_config[i+1][1], kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.layers.append(down_smpl)\n",
        "    \n",
        "    self.layers = torch.nn.Sequential(*self.layers)\n",
        "\n",
        "    self.embeddings = torch.nn.Sequential(\n",
        "        torch.nn.AdaptiveAvgPool2d(1),\n",
        "        torch.nn.BatchNorm2d(768),\n",
        "        torch.nn.Flatten(),\n",
        "    )\n",
        "\n",
        "    # Classification Layer\n",
        "    self.cls = torch.nn.Linear(768, self.num_classes)\n",
        "\n",
        "  def forward(self, x, return_feats=False):\n",
        "    \"\"\"\n",
        "    What is return_feats? It essentially returns the second-to-last-layer\n",
        "    features of a given image. It's a \"feature encoding\" of the input image,\n",
        "    and you can use it for the verification task. You would use the outputs\n",
        "    of the final classification layer for the classification task.\n",
        "\n",
        "    You might also find that the classification outputs are sometimes better\n",
        "    for verification too - try both.\n",
        "    \"\"\"\n",
        "    out = self.layers(x)\n",
        "    feats = self.embeddings(out)\n",
        "    out = self.cls(feats)\n",
        "    \n",
        "    if return_feats:\n",
        "      return feats\n",
        "    else:\n",
        "      return out \n",
        "\n",
        "model = Network()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model= torch.nn.DataParallel(model)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Setup everything for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.2) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=1,verbose=True)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * config['epochs']))\n",
        "# You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
        "scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM11HtcboYv"
      },
      "source": [
        "# Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgSw6iJJavBZ"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar \n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
        "    \n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        \n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it! \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() \n",
        "        scheduler.step()\n",
        "        # TODO? Depending on your choice of scheduler,\n",
        "        #scheduler.step(total_loss)   ############################################################################# should it be here ???????\n",
        "        # You may want to call some schdulers inside the train function. What are these?\n",
        "      \n",
        "        batch_bar.update() # Update tqdm bar\n",
        "    #scheduler.step(total_loss)   ################################################################################## it should be here !!!!!!!\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5V2UdnpdEoK"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "  \n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        \n",
        "        # Move images to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    #scheduler.step(total_loss)\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmotca6pcLLY",
        "outputId": "8936bc0a-7f81-4cf3-ffd4-33c6518bf0e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBgKGkXLrdJ"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix62_BkaLr_D",
        "outputId": "4f000579-14e9-439c-8aa9-82db470a2d09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"2178c9f0d96e90016c3d36bcccb07de5e0c51edc\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "15647e9021af41ea8366ce4d1d0272b1",
            "40faf1e73d5b4abfb9f0f70693753412",
            "9e7eb6e0fbfe42eca607527597f55045",
            "ec5b67f6afd04ec3bc090d2f0025ef7c",
            "a5ff904cd43044daa8c323c07581b1f4",
            "833576296d87451abcf6ff7d496039f5",
            "58d5fcb5179c48608bbed3bf6693201c",
            "fd84aed41d584aa1b9244126345f3ed1"
          ]
        },
        "id": "VG0vmsmbRYEi",
        "outputId": "9b439206-705e-4253-92d5-5aae147006c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15647e9021af41ea8366ce4d1d0272b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669057766678937, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error communicating with wandb process\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m For more info see: https://docs.wandb.ai/library/init#init-start-error\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Problem at: <ipython-input-88-083a376a9176> 8 <module>\n"
          ]
        },
        {
          "ename": "UsageError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUsageError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-083a376a9176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hw2p2-ablations-mid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m### Project should be created in your wandb account\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;31m### Wandb Config for your run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mrun_result\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrun_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrun_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUsageError\u001b[0m: Error communicating with wandb process\nFor more info see: https://docs.wandb.ai/library/init#init-start-error"
          ]
        }
      ],
      "source": [
        "# # Create your wandb run\n",
        "# run = wandb.init(\n",
        "#     name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "#     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "#     # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "#     # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "#     project = \"hw2p2-ablations-mid\", ### Project should be created in your wandb account \n",
        "#     config = config ### Wandb Config for your run\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkRw1FvLqYe"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqWO8Edb0BK2",
        "outputId": "e1c06559-7294-479b-b8e6-b96da5bac51f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/85: \n",
            "Train Acc 0.0557%\t Train Loss 8.7686\t Learning Rate 0.1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 0.1600%\t Val Loss 8.4159\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/85: \n",
            "Train Acc 0.5227%\t Train Loss 8.1969\t Learning Rate 0.1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 1.6453%\t Val Loss 7.7482\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/85: \n",
            "Train Acc 3.1093%\t Train Loss 7.4953\t Learning Rate 0.0999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 6.0672%\t Val Loss 7.1168\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/85: \n",
            "Train Acc 10.8211%\t Train Loss 6.7336\t Learning Rate 0.0997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 18.7214%\t Val Loss 6.2719\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/85: \n",
            "Train Acc 23.5289%\t Train Loss 6.0096\t Learning Rate 0.0995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 33.1753%\t Val Loss 5.5861\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/85: \n",
            "Train Acc 37.8699%\t Train Loss 5.3775\t Learning Rate 0.0991\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 43.2016%\t Val Loss 5.2370\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/85: \n",
            "Train Acc 49.3173%\t Train Loss 4.8969\t Learning Rate 0.0988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 51.0255%\t Val Loss 4.8582\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/85: \n",
            "Train Acc 57.6418%\t Train Loss 4.5529\t Learning Rate 0.0983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 57.1955%\t Val Loss 4.6079\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/85: \n",
            "Train Acc 63.9075%\t Train Loss 4.3025\t Learning Rate 0.0978\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 62.0915%\t Val Loss 4.4071\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/85: \n",
            "Train Acc 68.4501%\t Train Loss 4.1131\t Learning Rate 0.0973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 65.5250%\t Val Loss 4.3223\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11/85: \n",
            "Train Acc 71.8864%\t Train Loss 3.9625\t Learning Rate 0.0966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 66.7447%\t Val Loss 4.2059\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12/85: \n",
            "Train Acc 74.6708%\t Train Loss 3.8423\t Learning Rate 0.0959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 68.7414%\t Val Loss 4.1693\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13/85: \n",
            "Train Acc 77.3445%\t Train Loss 3.7257\t Learning Rate 0.0952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 71.3351%\t Val Loss 4.0631\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14/85: \n",
            "Train Acc 79.4461%\t Train Loss 3.6343\t Learning Rate 0.0943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 71.3437%\t Val Loss 4.0523\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15/85: \n",
            "Train Acc 80.7137%\t Train Loss 3.5690\t Learning Rate 0.0935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 71.2866%\t Val Loss 4.0389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16/85: \n",
            "Train Acc 82.2483%\t Train Loss 3.5011\t Learning Rate 0.0925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 72.5377%\t Val Loss 3.9682\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17/85: \n",
            "Train Acc 83.3581%\t Train Loss 3.4484\t Learning Rate 0.0915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 72.8291%\t Val Loss 3.9638\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18/85: \n",
            "Train Acc 84.5742%\t Train Loss 3.3956\t Learning Rate 0.0905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 74.1802%\t Val Loss 3.9304\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19/85: \n",
            "Train Acc 85.4226%\t Train Loss 3.3494\t Learning Rate 0.0893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 74.6258%\t Val Loss 3.9332\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20/85: \n",
            "Train Acc 86.9744%\t Train Loss 3.2909\t Learning Rate 0.0882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 73.4146%\t Val Loss 3.9396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 21/85: \n",
            "Train Acc 87.4236%\t Train Loss 3.2629\t Learning Rate 0.0870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 73.8431%\t Val Loss 3.9325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 22/85: \n",
            "Train Acc 88.1034%\t Train Loss 3.2354\t Learning Rate 0.0857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 75.2628%\t Val Loss 3.8686\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 23/85: \n",
            "Train Acc 89.5910%\t Train Loss 3.1740\t Learning Rate 0.0844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 75.5285%\t Val Loss 3.8848\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 24/85: \n",
            "Train Acc 89.9944%\t Train Loss 3.1567\t Learning Rate 0.0830\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 75.7684%\t Val Loss 3.8785\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 25/85: \n",
            "Train Acc 90.8414%\t Train Loss 3.1223\t Learning Rate 0.0816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 76.4768%\t Val Loss 3.8836\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 26/85: \n",
            "Train Acc 91.6169%\t Train Loss 3.0907\t Learning Rate 0.0801\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 76.0283%\t Val Loss 3.8630\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 27/85: \n",
            "Train Acc 91.8733%\t Train Loss 3.0763\t Learning Rate 0.0786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 76.3854%\t Val Loss 3.8245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 28/85: \n",
            "Train Acc 92.5181%\t Train Loss 3.0440\t Learning Rate 0.0771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 76.3397%\t Val Loss 3.8375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 29/85: \n",
            "Train Acc 93.1801%\t Train Loss 3.0176\t Learning Rate 0.0755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 77.9793%\t Val Loss 3.7798\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 30/85: \n",
            "Train Acc 93.3672%\t Train Loss 3.0091\t Learning Rate 0.0739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 77.2252%\t Val Loss 3.8026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 31/85: \n",
            "Train Acc 93.8907%\t Train Loss 2.9878\t Learning Rate 0.0723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 77.1452%\t Val Loss 3.7873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 32/85: \n",
            "Train Acc 94.5605%\t Train Loss 2.9543\t Learning Rate 0.0706\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 79.2019%\t Val Loss 3.7299\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 33/85: \n",
            "Train Acc 95.0704%\t Train Loss 2.9304\t Learning Rate 0.0689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 76.9367%\t Val Loss 3.8138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 34/85: \n",
            "Train Acc 95.1754%\t Train Loss 2.9217\t Learning Rate 0.0672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 77.9479%\t Val Loss 3.7886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 35/85: \n",
            "Train Acc 95.2318%\t Train Loss 2.9168\t Learning Rate 0.0655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 77.2481%\t Val Loss 3.8190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 36/85: \n",
            "Train Acc 95.7260%\t Train Loss 2.8975\t Learning Rate 0.0637\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 79.1562%\t Val Loss 3.7325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 37/85: \n",
            "Train Acc 96.4158%\t Train Loss 2.8619\t Learning Rate 0.0619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 78.4221%\t Val Loss 3.7465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 38/85: \n",
            "Train Acc 96.8007%\t Train Loss 2.8415\t Learning Rate 0.0601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 80.3445%\t Val Loss 3.6584\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 39/85: \n",
            "Train Acc 96.7922%\t Train Loss 2.8351\t Learning Rate 0.0583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 80.2959%\t Val Loss 3.6791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 40/85: \n",
            "Train Acc 97.1014%\t Train Loss 2.8260\t Learning Rate 0.0564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 80.8844%\t Val Loss 3.6632\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 41/85: \n",
            "Train Acc 97.4113%\t Train Loss 2.8003\t Learning Rate 0.0546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 81.8441%\t Val Loss 3.6145\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 42/85: \n",
            "Train Acc 97.6770%\t Train Loss 2.7831\t Learning Rate 0.0528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 81.5642%\t Val Loss 3.6615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 43/85: \n",
            "Train Acc 97.9055%\t Train Loss 2.7672\t Learning Rate 0.0509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 82.4297%\t Val Loss 3.5882\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 44/85: \n",
            "Train Acc 98.0876%\t Train Loss 2.7576\t Learning Rate 0.0491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 82.0755%\t Val Loss 3.6370\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 45/85: \n",
            "Train Acc 98.2325%\t Train Loss 2.7432\t Learning Rate 0.0472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 81.0500%\t Val Loss 3.7203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 46/85: \n",
            "Train Acc 98.4096%\t Train Loss 2.7284\t Learning Rate 0.0454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 83.4295%\t Val Loss 3.5857\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 47/85: \n",
            "Train Acc 98.7331%\t Train Loss 2.7035\t Learning Rate 0.0436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 83.0724%\t Val Loss 3.6638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 48/85: \n",
            "Train Acc 98.8260%\t Train Loss 2.6886\t Learning Rate 0.0417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 82.2298%\t Val Loss 3.6020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 49/85: \n",
            "Train Acc 99.0245%\t Train Loss 2.6711\t Learning Rate 0.0399\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 83.6894%\t Val Loss 3.5249\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 50/85: \n",
            "Train Acc 99.1795%\t Train Loss 2.6571\t Learning Rate 0.0381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 84.4121%\t Val Loss 3.5890\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 51/85: \n",
            "Train Acc 99.1723%\t Train Loss 2.6491\t Learning Rate 0.0363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 83.3181%\t Val Loss 3.5594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 52/85: \n",
            "Train Acc 99.3009%\t Train Loss 2.6347\t Learning Rate 0.0345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 83.8980%\t Val Loss 3.4950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 53/85: \n",
            "Train Acc 99.3844%\t Train Loss 2.6208\t Learning Rate 0.0328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 85.6033%\t Val Loss 3.4668\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 54/85: \n",
            "Train Acc 99.4930%\t Train Loss 2.6060\t Learning Rate 0.0311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 85.6947%\t Val Loss 3.4575\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 55/85: \n",
            "Train Acc 99.5130%\t Train Loss 2.5964\t Learning Rate 0.0294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 85.8889%\t Val Loss 3.4836\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 56/85: \n",
            "Train Acc 99.4844%\t Train Loss 2.5900\t Learning Rate 0.0277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 86.9116%\t Val Loss 3.4182\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 57/85: \n",
            "Train Acc 99.6908%\t Train Loss 2.5624\t Learning Rate 0.0261\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 86.4174%\t Val Loss 3.5889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 58/85: \n",
            "Train Acc 99.7458%\t Train Loss 2.5521\t Learning Rate 0.0245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 86.2403%\t Val Loss 3.5790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 59/85: \n",
            "Train Acc 99.7151%\t Train Loss 2.5479\t Learning Rate 0.0229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 86.8087%\t Val Loss 3.4424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 60/85: \n",
            "Train Acc 99.7579%\t Train Loss 2.5340\t Learning Rate 0.0214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 87.6228%\t Val Loss 3.3833\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 61/85: \n",
            "Train Acc 99.8372%\t Train Loss 2.5162\t Learning Rate 0.0199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 87.7657%\t Val Loss 3.3776\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 62/85: \n",
            "Train Acc 99.8329%\t Train Loss 2.5078\t Learning Rate 0.0184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 87.9399%\t Val Loss 3.3050\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 63/85: \n",
            "Train Acc 99.8572%\t Train Loss 2.4983\t Learning Rate 0.0170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 87.9256%\t Val Loss 3.3555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 64/85: \n",
            "Train Acc 99.8736%\t Train Loss 2.4852\t Learning Rate 0.0156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 88.3798%\t Val Loss 3.4016\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 65/85: \n",
            "Train Acc 99.8807%\t Train Loss 2.4791\t Learning Rate 0.0143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 88.3655%\t Val Loss 3.4162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 66/85: \n",
            "Train Acc 99.8986%\t Train Loss 2.4722\t Learning Rate 0.0130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 89.1853%\t Val Loss 3.3963\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 67/85: \n",
            "Train Acc 99.8915%\t Train Loss 2.4645\t Learning Rate 0.0118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 88.5426%\t Val Loss 3.3730\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 68/85: \n",
            "Train Acc 99.8915%\t Train Loss 2.4596\t Learning Rate 0.0107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 89.1968%\t Val Loss 3.2657\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 69/85: \n",
            "Train Acc 99.9150%\t Train Loss 2.4512\t Learning Rate 0.0095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 89.6081%\t Val Loss 3.2771\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 70/85: \n",
            "Train Acc 99.9179%\t Train Loss 2.4443\t Learning Rate 0.0085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 89.2339%\t Val Loss 3.4945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 71/85: \n",
            "Train Acc 99.9207%\t Train Loss 2.4406\t Learning Rate 0.0075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 89.7052%\t Val Loss 3.3680\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 72/85: \n",
            "Train Acc 99.9022%\t Train Loss 2.4330\t Learning Rate 0.0065\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 89.6681%\t Val Loss 3.2729\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 73/85: \n",
            "Train Acc 99.9443%\t Train Loss 2.4263\t Learning Rate 0.0057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 89.9166%\t Val Loss 3.2779\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 74/85: \n",
            "Train Acc 99.9293%\t Train Loss 2.4226\t Learning Rate 0.0048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.2165%\t Val Loss 3.2301\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 75/85: \n",
            "Train Acc 99.9429%\t Train Loss 2.4202\t Learning Rate 0.0041\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.1423%\t Val Loss 3.2809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 76/85: \n",
            "Train Acc 99.9364%\t Train Loss 2.4225\t Learning Rate 0.0034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.5450%\t Val Loss 3.3281\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 77/85: \n",
            "Train Acc 99.9593%\t Train Loss 2.4125\t Learning Rate 0.0027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.5336%\t Val Loss 3.2894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 78/85: \n",
            "Train Acc 99.9579%\t Train Loss 2.4081\t Learning Rate 0.0022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.3708%\t Val Loss 3.2845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 79/85: \n",
            "Train Acc 99.9264%\t Train Loss 2.4147\t Learning Rate 0.0017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.2251%\t Val Loss 3.2315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 80/85: \n",
            "Train Acc 99.8822%\t Train Loss 2.4123\t Learning Rate 0.0012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.3993%\t Val Loss 3.2295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 81/85: \n",
            "Train Acc 99.9629%\t Train Loss 2.4064\t Learning Rate 0.0009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.4593%\t Val Loss 3.2342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 82/85: \n",
            "Train Acc 99.9393%\t Train Loss 2.4075\t Learning Rate 0.0005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.6564%\t Val Loss 3.2267\n",
            "Saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 83/85: \n",
            "Train Acc 99.9407%\t Train Loss 2.4061\t Learning Rate 0.0003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.5764%\t Val Loss 3.3279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 84/85: \n",
            "Train Acc 99.9229%\t Train Loss 2.4059\t Learning Rate 0.0001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.5536%\t Val Loss 3.2220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 85/85: \n",
            "Train Acc 99.9493%\t Train Loss 2.4068\t Learning Rate 0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                      "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Acc 90.6021%\t Val Loss 3.2390\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "best_valacc = 0.0\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    \n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        config['epochs'],\n",
        "        #10,\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr))\n",
        "    \n",
        "    val_acc, val_loss = validate(model, val_loader, criterion)\n",
        "    #scheduler.step(val_loss)\n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "\n",
        "    # wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, \n",
        "    #           'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "    \n",
        "    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n",
        "    # your learning rate differently \n",
        "\n",
        "    # #Save model in drive location if val_acc is better than best recorded val_acc\n",
        "    if (val_acc >= best_valacc):\n",
        "      #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "      print(\"Saving model\")\n",
        "      torch.save({'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  'scheduler_state_dict':scheduler.state_dict(),\n",
        "                  'val_acc': val_acc, \n",
        "                  'epoch': epoch}, './checkpoint.pth')\n",
        "      best_valacc = val_acc\n",
        "      # wandb.save('checkpoint.pth')\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2WQEUjXkWvo"
      },
      "outputs": [],
      "source": [
        "def test(model,dataloader):\n",
        "\n",
        "  model.eval()\n",
        "  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "  test_results = []\n",
        "  \n",
        "  for i, (images) in enumerate(dataloader):\n",
        "      # TODO: Finish predicting on the test set.\n",
        "      images = images.to(device)\n",
        "\n",
        "      with torch.inference_mode():\n",
        "        outputs = model(images)\n",
        "\n",
        "      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n",
        "      test_results.extend(outputs)\n",
        "      \n",
        "      batch_bar.update()\n",
        "      \n",
        "  batch_bar.close()\n",
        "  return test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7R1lcCAzULc",
        "outputId": "e9d9b9fb-a655-4b64-835b-792861955adf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "test_results = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqfUzwS2L1gx"
      },
      "source": [
        "## Generate csv to submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "with open(\"classification_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qnf6dYiB6HC",
        "outputId": "a91210af-66fc-412c-f965-80161db6ede3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 541k/541k [00:02<00:00, 241kB/s]\n",
            "Successfully submitted to Face Recognition (Slack)"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-f22-hw2p2-classification-slack -f ./classification_submission.csv -m \"Low-Cutoff Submission\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJx1l1T4twC"
      },
      "source": [
        "# Verification Task: Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      },
      "source": [
        "The verification task consists of the following generalized scenario:\n",
        "- You are given X unknown identitites \n",
        "- You are given Y known identitites\n",
        "- Your goal is to match X unknown identities to Y known identities.\n",
        "\n",
        "We have given you a verification dataset, that consists of 1000 known identities, and 1000 unknown identities. The 1000 unknown identities are split into dev (200) and test (800). Your goal is to compare the unknown identities to the 1000 known identities and assign an identity to each image from the set of unknown identities. \n",
        "\n",
        "Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities. \n",
        "\n",
        "This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9aY5o-suWdn",
        "outputId": "b0300899-5538-4d6c-d823-ead57514da6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 5208.02it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 2551.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n",
            "1000\n",
            "torch.Size([200, 3, 224, 224])\n",
            "torch.Size([1000, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "known_regex = \"/content/data/verification/known/*/*\"\n",
        "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))] \n",
        "# This obtains the list of known identities from the known folder\n",
        "\n",
        "### Validation Directory\n",
        "unknown_regex = \"/content/data/verification/unknown_dev/*\"\n",
        "\n",
        "### Test Directory\n",
        "#unknown_regex = \"/content/data/verification/unknown_test/*\"\n",
        "\n",
        "# We load the images from known and unknown folders\n",
        "unknown_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_regex)))]\n",
        "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
        "\n",
        "print (len(unknown_images))\n",
        "print (len(known_images))\n",
        "# Why do you need only ToTensor() here?\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()])\n",
        "\n",
        "unknown_images = torch.stack([transforms(x) for x in unknown_images])\n",
        "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
        "#Print your shapes here to understand what we have done\n",
        "print (unknown_images.size())\n",
        "print (known_images.size())\n",
        "# You can use other similarity metrics like Euclidean Distance if you wish\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk1LS0BRxFHM"
      },
      "outputs": [],
      "source": [
        "def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'): \n",
        "\n",
        "    unknown_feats, known_feats = [], []\n",
        "\n",
        "    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    model.eval()\n",
        "\n",
        "    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n",
        "    for i in range(0, unknown_images.shape[0], batch_size):\n",
        "        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            unknown_feat = model(unknown_batch.float().to(device), return_feats=True) #Get features from model         \n",
        "        unknown_feats.append(unknown_feat)\n",
        "        batch_bar.update()\n",
        "    \n",
        "    batch_bar.close()\n",
        "    \n",
        "    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    \n",
        "    for i in range(0, known_images.shape[0], batch_size):\n",
        "        known_batch = known_images[i:i+batch_size] \n",
        "        with torch.no_grad():\n",
        "              known_feat = model(known_batch.float().to(device), return_feats=True)\n",
        "          \n",
        "        known_feats.append(known_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    # Concatenate all the batches\n",
        "    unknown_feats = torch.cat(unknown_feats, dim=0)\n",
        "    known_feats = torch.cat(known_feats, dim=0)\n",
        "\n",
        "    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n",
        "    # Print the inner list comprehension in a separate cell - what is really happening?\n",
        "    print(f\"similarity_values: {len(similarity_values)}\")\n",
        "\n",
        "    predictions = similarity_values.argmax(0).cpu().numpy() #Why are we doing an argmax here?\n",
        "    print(f\"predictions number: {len(predictions)}\")\n",
        "    # Map argmax indices to identity strings\n",
        "    pred_id_strings = [known_paths[i] for i in predictions]\n",
        "    \n",
        "    if mode == 'val':\n",
        "      true_ids = pd.read_csv('/content/data/verification/dev_identities.csv')['label'].tolist()\n",
        "      print(f\"true_ids: {len(true_ids)}\")\n",
        "      print(f\"pred_id_strings: {len(pred_id_strings)}\")\n",
        "      accuracy = accuracy_score(pred_id_strings, true_ids)\n",
        "      print(\"Verification Accuracy = {}\".format(accuracy))\n",
        "    \n",
        "    return pred_id_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMC7FacaUnJ7",
        "outputId": "90904b1f-9cf9-42e2-e377-0a5e895d29bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "similarity_values: 1000\n",
            "predictions number: 200\n",
            "true_ids: 200\n",
            "pred_id_strings: 200\n",
            "Verification Accuracy = 0.67\n"
          ]
        }
      ],
      "source": [
        "# If we are validating\n",
        "pred_id_strings = eval_verification(unknown_images, known_images, model, similarity_metric, config['batch_size'], mode='val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_DbhfjHS2fm",
        "outputId": "be12bd23-7899-47bb-c9ba-bcaf8e414e28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 2657.22it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 5128.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "800\n",
            "1000\n",
            "torch.Size([800, 3, 224, 224])\n",
            "torch.Size([1000, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "known_regex = \"/content/data/verification/known/*/*\"\n",
        "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))] \n",
        "# This obtains the list of known identities from the known folder\n",
        "\n",
        "### Validation Directory\n",
        "#unknown_regex = \"/content/data/verification/unknown_dev/*\"\n",
        "\n",
        "### Test Directory\n",
        "unknown_regex = \"/content/data/verification/unknown_test/*\"\n",
        "\n",
        "# We load the images from known and unknown folders\n",
        "unknown_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_regex)))]\n",
        "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
        "\n",
        "print (len(unknown_images))\n",
        "print (len(known_images))\n",
        "# Why do you need only ToTensor() here?\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()])\n",
        "\n",
        "unknown_images = torch.stack([transforms(x) for x in unknown_images])\n",
        "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
        "#Print your shapes here to understand what we have done\n",
        "print (unknown_images.size())\n",
        "print (known_images.size())\n",
        "# You can use other similarity metrics like Euclidean Distance if you wish\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOWBJ1D7P0Lp",
        "outputId": "4d13fcea-4bd7-464e-a5a0-4b40833b85ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "similarity_values: 1000\n",
            "predictions number: 800\n"
          ]
        }
      ],
      "source": [
        "# If we are testing\n",
        "pred_id_strings = eval_verification(unknown_images, known_images, model, similarity_metric, config['batch_size'], mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD-r-HmsAeWV"
      },
      "outputs": [],
      "source": [
        "with open(\"verification_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(pred_id_strings)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlqer5cmDEMC",
        "outputId": "f8070504-a40b-427e-bd20-df22be50fcf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 9.28k/9.28k [00:02<00:00, 4.55kB/s]\n",
            "Successfully submitted to Face Verification (Slack)"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-f22-hw2p2-verification-slack -f ./verification_submission.csv -m \"Low-Cutoff Submission\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15647e9021af41ea8366ce4d1d0272b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e7eb6e0fbfe42eca607527597f55045",
              "IPY_MODEL_ec5b67f6afd04ec3bc090d2f0025ef7c"
            ],
            "layout": "IPY_MODEL_40faf1e73d5b4abfb9f0f70693753412"
          }
        },
        "40faf1e73d5b4abfb9f0f70693753412": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d5fcb5179c48608bbed3bf6693201c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "833576296d87451abcf6ff7d496039f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e7eb6e0fbfe42eca607527597f55045": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_833576296d87451abcf6ff7d496039f5",
            "placeholder": "​",
            "style": "IPY_MODEL_a5ff904cd43044daa8c323c07581b1f4",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "a5ff904cd43044daa8c323c07581b1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec5b67f6afd04ec3bc090d2f0025ef7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd84aed41d584aa1b9244126345f3ed1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58d5fcb5179c48608bbed3bf6693201c",
            "value": 0.9844938773666703
          }
        },
        "fd84aed41d584aa1b9244126345f3ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
